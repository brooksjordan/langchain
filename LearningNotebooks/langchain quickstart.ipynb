{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf5aa88",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "LangChain has a number of components designed to help build question-answering applications, and RAG applications more generally. \n",
    "\n",
    "To familiarize ourselves with these, we'll build a simple Q&A application over a text data source. Along the way we'll go over a typical Q&A architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced Q&A techniques. \n",
    "\n",
    "We'll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931068f",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "We'll create a typical RAG application as outlined in the Q&A introduction, which has two main components:\n",
    "\n",
    "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The full sequence from raw data to answer will look like:\n",
    "\n",
    "## Indexing\n",
    "\n",
    "1. Load: First we need to load our data. We'll use DocumentLoaders for this.\n",
    "\n",
    "2. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "\n",
    "3. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "## Retrieval and generation\n",
    "\n",
    "4. Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "\n",
    "5. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e3c13",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "We'll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.\n",
    "\n",
    "We'll use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8481fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "# Any other imports or simple tests you want to perform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10defb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
